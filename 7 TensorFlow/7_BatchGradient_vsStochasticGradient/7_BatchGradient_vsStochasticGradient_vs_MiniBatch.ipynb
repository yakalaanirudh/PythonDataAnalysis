{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d90de7-83d9-4383-b270-16d48bef52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of stochastic and batch grandient descent in python\n",
    "\n",
    "\"\"\"\n",
    "We will use very simple home prices data set to implement batch and stochastic gradient descent in python. \n",
    "Batch gradient descent uses all training samples in forward pass to calculate cumulitive error and than \n",
    "we adjust weights using derivaties. In stochastic GD, we randomly pick one training sample, perform forward pass, \n",
    "compute the error and immidiately adjust weights. So the key difference here is that to adjust weights batch GD \n",
    "will use all training samples where as stochastic GD will use one randomly picked training sample\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab3bef-9480-4a6b-bdbf-922d21701e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d124d-bbce-4bf3-83d6-b5e9d43d574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"homeprices_banglore.csv\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275a276-31f1-45c2-8531-07b278e586a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#x is area and bedrooms and y is price\n",
    "sx = preprocessing.MinMaxScaler()\n",
    "sy = preprocessing.MinMaxScaler()\n",
    "\n",
    "#scaled x is the dataframe we get by dropping price from the main df\n",
    "scaled_X = sx.fit_transform(df.drop('price',axis='columns'))\n",
    "#scaled y is the dataframe we get from price column of main df we reshape the array\n",
    "#as a 2d df cause minmaxscaler needs a 2d df, here rows is numbe rof rows in main df\n",
    "#and cols is 1\n",
    "scaled_y = sy.fit_transform(df['price'].values.reshape(df.shape[0],1))\n",
    "\n",
    "scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d613ca-0a17-402d-a09e-79e378a83261",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a0b90-88a6-4087-b54c-4e55d28fa636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We should convert target column (i.e. price) into one dimensional array. \n",
    "#It has become 2D due to scaling that we did above but now we should change to 1D\n",
    "scaled_y.reshape(20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a795483-8b48-477d-9caf-e96c3a4f3851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent Implementation\n",
    "\n",
    "def batch_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
    "\n",
    "    number_of_features = X.shape[1] [60000,2] so number of features =2\n",
    "    # numpy array with 1 row and columns equal to number of features. In \n",
    "    # our case number_of_features = 2 (area, bedroom)\n",
    "    w = np.ones(shape=(number_of_features)) #(1,1)\n",
    "    b = 0\n",
    "    total_samples = X.shape[0] # number of rows in X\n",
    "\n",
    "    #two lists to push the cost value and epoch number\n",
    "    cost_list = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for i in range(epochs):        \n",
    "        # price is w1*sqft+w2*numbe rof rooms +b\n",
    "        #Matrix multiplication of dot product between w and X.T, resulting in a predicted output.\n",
    "        y_predicted = np.dot(w, X.T) + b\n",
    "\n",
    "        #Gradient of the Loss w.r.t w (Weight Update)\n",
    "        w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\n",
    "        b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\n",
    "\n",
    "        #we reduce the wgrad in accordance to learning rate\n",
    "        w = w - learning_rate * w_grad\n",
    "        b = b - learning_rate * b_grad\n",
    "\n",
    "        #calculating mse loss\n",
    "        cost = np.mean(np.square(y_true-y_predicted)) # MSE (Mean Squared Error)\n",
    "\n",
    "        #if epoch divisible by 10 oush epoch and loss to arrays for plotting\n",
    "        if i%10==0:\n",
    "            cost_list.append(cost)\n",
    "            epoch_list.append(i)\n",
    "        \n",
    "    return w, b, cost, cost_list, epoch_list\n",
    "\n",
    "w, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),500)\n",
    "w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786574c-b7ef-43be-a6bf-e9614c42d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.plot(epoch_list,cost_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea79179a-1ea1-4693-9c1d-5cf96700d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A prediction\n",
    "\n",
    "def predict(area,bedrooms,w,b):\n",
    "    scaled_X = sx.transform([[area, bedrooms]])[0]\n",
    "    scaled_price = w[0] * scaled_X[0] + w[1] * scaled_X[1] + b\n",
    "    # once we get price prediction we need to to rescal it back to original value\n",
    "    # also since it returns 2D array, to get single value we need to do value[0][0]\n",
    "    return sy.inverse_transform([[scaled_price]])[0][0]\n",
    "\n",
    "predict(2600,4,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd192ab8-2b1c-4d9c-a61e-a85b89f4a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(1000,2,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c51bc1-d370-40be-ac57-25b9b4e827f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(1500,3,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63faea08-6e49-4bd3-af5f-aa0643b27a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent Implementation\n",
    "\n",
    "# we will use random libary to pick random training sample.\n",
    "import random\n",
    "random.randint(0,6) # randit gives random number between two numbers specified in the argument\n",
    "\n",
    "def stochastic_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
    " \n",
    "    number_of_features = X.shape[1]\n",
    "    # numpy array with 1 row and columns equal to number of features. In \n",
    "    # our case number_of_features =2 (area, bedroom)\n",
    "    w = np.ones(shape=(number_of_features)) \n",
    "    b = 0\n",
    "    total_samples = X.shape[0]\n",
    "    \n",
    "    cost_list = []\n",
    "    epoch_list = []\n",
    "\n",
    "    #this means over the all epoch values select a value and selct its x and y respectively\n",
    "    for i in range(epochs):    \n",
    "        random_index = random.randint(0,total_samples-1) # random index from total samples\n",
    "        sample_x = X[random_index]\n",
    "        sample_y = y_true[random_index]\n",
    "        \n",
    "        y_predicted = np.dot(w, sample_x.T) + b\n",
    "    \n",
    "        w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\n",
    "        b_grad = -(2/total_samples)*(sample_y-y_predicted)\n",
    "        \n",
    "        w = w - learning_rate * w_grad\n",
    "        b = b - learning_rate * b_grad\n",
    "        \n",
    "        cost = np.square(sample_y-y_predicted)\n",
    "        \n",
    "        if i%100==0: # at every 100th iteration record the cost and epoch value\n",
    "            cost_list.append(cost)\n",
    "            epoch_list.append(i)\n",
    "        \n",
    "    return w, b, cost, cost_list, epoch_list\n",
    "\n",
    "w_sgd, b_sgd, cost_sgd, cost_list_sgd, epoch_list_sgd = stochastic_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)\n",
    "w_sgd, b_sgd, cost_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94942bf5-e543-421a-9101-ba16fa2acc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini-Batch Gradient Descent Implementation\n",
    "np.random.permutation(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5963bd-67f3-4679-a98c-70be848297b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y_true, epochs = 100, batch_size = 5, learning_rate = 0.01):\n",
    "    \n",
    "    number_of_features = X.shape[1]\n",
    "    w = np.ones(shape=(number_of_features)) \n",
    "    b = 0\n",
    "    total_samples = X.shape[0] # number of rows in X\n",
    "\n",
    "    #if samples is less than batch size take all sample\n",
    "    if batch_size > total_samples: \n",
    "        batch_size = total_samples\n",
    "        \n",
    "    cost_list = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    num_batches = int(total_samples/batch_size)\n",
    "    \"\"\"\n",
    "    num_batches determines how many batches will be created per epoch.\n",
    "    If total_samples = 1000 and batch_size = 50, then num_batches = 1000 / 50 = 20.\n",
    "    This means each epoch processes 20 mini-batches.\n",
    "\n",
    "    np.random.permutation(total_samples): Generates a random order of indices.\n",
    "    X_tmp = X[random_indices]: Shuffles X using the new random order.\n",
    "    y_tmp = y_true[random_indices]: Shuffles y_true in the same way.\n",
    "    Shuffling ensures that each epoch gets different mini-batches, preventing the model from learning in a fixed order.\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(epochs):    \n",
    "        random_indices = np.random.permutation(total_samples)\n",
    "        X_tmp = X[random_indices]\n",
    "        y_tmp = y_true[random_indices]\n",
    "        \n",
    "        for j in range(0,total_samples,batch_size):\n",
    "            Xj = X_tmp[j:j+batch_size]\n",
    "            yj = y_tmp[j:j+batch_size]\n",
    "            y_predicted = np.dot(w, Xj.T) + b\n",
    "            \n",
    "            w_grad = -(2/len(Xj))*(Xj.T.dot(yj-y_predicted))\n",
    "            b_grad = -(2/len(Xj))*np.sum(yj-y_predicted)\n",
    "            \n",
    "            w = w - learning_rate * w_grad\n",
    "            b = b - learning_rate * b_grad\n",
    "                \n",
    "            cost = np.mean(np.square(yj-y_predicted)) # MSE (Mean Squared Error)\n",
    "        \n",
    "        if i%10==0:\n",
    "            cost_list.append(cost)\n",
    "            epoch_list.append(i)\n",
    "        \n",
    "    return w, b, cost, cost_list, epoch_list\n",
    "\n",
    "w, b, cost, cost_list, epoch_list = \n",
    "mini_batch_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),epochs = 120,batch_size = 5)\n",
    "\n",
    "w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b1101-6381-4cc7-9758-5ae8253e83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.plot(epoch_list,cost_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bf385-b2b6-4ed4-862e-97b5b62986c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
