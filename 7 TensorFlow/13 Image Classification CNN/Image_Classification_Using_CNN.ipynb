{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabeea4-1970-49e4-a3e6-206a9426c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handwritten digits classification using CNN\n",
    "\n",
    "#Here we can comapre results with ANN and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a169c206-1239-4edb-af39-36630023917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f5f944-0608-4391-8724-97e644b16dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f3c347-5138-41a0-8b6f-857c0e9d2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# Means 60000 images and each image is 28/28 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b911aec2-b628-4c3c-b011-33bb17918c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e37a84-dfcb-4414-84f1-0e9a24115249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f5cd1b-11cb-4642-90af-6735d7d3aa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64487b12-4e03-4eb2-9743-97293b294021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093d38f1-6803-4b2f-bd33-68e45213176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since rgb values range frpm 0 to 255  we scale them\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a4161c-48ea-474c-9435-4ee6d4069e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.4438\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.1285\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0818\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0636\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0487\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0384\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0313\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0262\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0218\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1afd744af60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using ANN for classification\n",
    "\n",
    "\"\"\"\n",
    "Here we have 3 layers\n",
    "first to flatten - The Flatten layer does not count as a neuron layer since it only reshapes the input.\n",
    "second is the first dense layer\n",
    "third is the output layer\n",
    "\"\"\"\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d65a66a-98c5-45a7-8db5-3b2cade0efa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9709 - loss: 0.1087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09047912806272507, 0.975600004196167]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0967a19-6f2a-41bc-b363-47e48abd6cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train = X_train.reshape(-1, 28, 28, 1)\\nreshapes the dataset X_train, changing its dimensions.\\n\\n-1: This lets NumPy automatically determine the number of samples (it remains 60000).\\n28, 28: This keeps each image as a 28×28 grid (height × width).\\n1: This adds a channel dimension (for grayscale images).\\n\\nBefore and After Shape:\\nBefore reshaping: (60000, 28, 28) → Each image is stored as a 2D array.\\nAfter reshaping: (60000, 28, 28, 1) → Now each image is stored as a 3D array with a single channel.\\n\\nMany deep learning frameworks (like TensorFlow/Keras) expect image data in a 4D format:\\n(batch_size, height, width, channels)\\nIf your images were RGB (colored), you’d have 3 channels ((60000, 28, 28, 3) for Red, Green, and Blue).\\nSince these are grayscale images, we use 1 channel.\\n\\nIn Convolutional Neural Networks (CNNs) (Conv2D layers in Keras), they require images in (height, width, channels) format.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "reshapes the dataset X_train, changing its dimensions.\n",
    "\n",
    "-1: This lets NumPy automatically determine the number of samples (it remains 60000).\n",
    "28, 28: This keeps each image as a 28×28 grid (height × width).\n",
    "1: This adds a channel dimension (for grayscale images).\n",
    "\n",
    "Before and After Shape:\n",
    "Before reshaping: (60000, 28, 28) → Each image is stored as a 2D array.\n",
    "After reshaping: (60000, 28, 28, 1) → Now each image is stored as a 3D array with a single channel.\n",
    "\n",
    "Many deep learning frameworks (like TensorFlow/Keras) expect image data in a 4D format:\n",
    "(batch_size, height, width, channels)\n",
    "If your images were RGB (colored), you’d have 3 channels ((60000, 28, 28, 3) for Red, Green, and Blue).\n",
    "Since these are grayscale images, we use 1 channel.\n",
    "\n",
    "In Convolutional Neural Networks (CNNs) (Conv2D layers in Keras), they require images in (height, width, channels) format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a34f82bb-394b-41c4-a8db-aea4eeb81bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bff172a-cd11-4a6b-aa20-523fd9b152b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ce9c46-3a35-403d-98f1-3b70cc2e6c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Using CNN for classification\n",
    "\n",
    "\"\"\"\n",
    "Conv2D(30, (3,3)):\n",
    "This adds 30 filters (neurons), each of size 3×3.\n",
    "Each filter slides over the input image and extracts features like edges, textures, or patterns.\n",
    "activation='relu':\n",
    "Applies the ReLU (Rectified Linear Unit) activation function, making the network more efficient in learning complex patterns.\n",
    "input_shape=(28, 28, 1):\n",
    "The input is a grayscale image of size 28×28 with 1 channel.\n",
    "\n",
    "MaxPooling2D((2,2)):\n",
    "Reduces the image size by taking the maximum value in every 2×2 region.\n",
    "This helps reduce the number of parameters and prevents overfitting.\n",
    "After Conv2D + MaxPooling, the output size is smaller but retains important features.\n",
    "\n",
    "Converts the 2D feature maps into a 1D vector so they can be fed into the fully connected layers.\n",
    "\n",
    "100 neurons with ReLU activation.\n",
    "It learns complex patterns from the extracted features.\n",
    "\n",
    "10 neurons, one for each output class (assuming a classification task with 10 categories 0 to 9\n",
    "\"\"\"\n",
    "model = keras.Sequential([\n",
    "    \n",
    "    layers.Conv2D(30, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    " \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0423b77-02a1-414f-9834-43da58c1364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 14ms/step - accuracy: 0.9040 - loss: 0.3165\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - accuracy: 0.9822 - loss: 0.0579\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - accuracy: 0.9885 - loss: 0.0382\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.9929 - loss: 0.0223\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - accuracy: 0.9950 - loss: 0.0162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1afd3e37bf0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5cc87c3-59be-47c1-b1c7-c512a93df264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa046dea-56f3-4524-a413-3bc24d1bbdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9836 - loss: 0.0550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0465860553085804, 0.9861999750137329]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa869a-d3e7-4f29-ab0e-bc81229b0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can see that \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
